{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Processing\n",
    "\n",
    "data preprocessing involves the following operations:\n",
    "1. dealing with missing value\n",
    "2. normalization\n",
    "3. standardization\n",
    "4. formatting\n",
    "5. binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../Datasets/cupcake.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mese</th>\n",
       "      <th>Cupcake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004-01</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004-03</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004-04</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004-05</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Mese  Cupcake\n",
       "0  2004-01      5.0\n",
       "1  2004-02      NaN\n",
       "2  2004-03      4.0\n",
       "3  2004-04      6.0\n",
       "4  2004-05      5.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn library provides two mechanisms to deal with missing values:\n",
    "1. univariate feature imputation\n",
    "2. multivariate feature imputation\n",
    "3. Nearest neighbors imputation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univaritate feature imputation\n",
    "\n",
    "- it involves the replacement of missing values with a constant value or some provided statistics related to a feature.\n",
    "- the SimpleImputer class can be used to perform univariate feature imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import nan\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "preprocessor = SimpleImputer(missing_values=np.nan, strategy='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleImputer()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(df['Cupcake']).reshape(-1,1)\n",
    "preprocessor.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleImputer()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SimpleImputer(add_indicator=False, copy=True, fill_value=None, missing_values=nan, strategy='mean', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prep = preprocessor.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mese</th>\n",
       "      <th>Cupcake</th>\n",
       "      <th>Cupcake_univariate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004-01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.079208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004-03</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004-04</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004-05</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Mese  Cupcake  Cupcake_univariate\n",
       "0  2004-01      5.0            5.000000\n",
       "1  2004-02      NaN           50.079208\n",
       "2  2004-03      4.0            4.000000\n",
       "3  2004-04      6.0            6.000000\n",
       "4  2004-05      5.0            5.000000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Cupcake_univariate'] = X_prep.reshape(1,-1)[0]\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multivariate Feature Imputation\n",
    "\n",
    "In the multivariate feature imputation each feature with missing values is calculated as a function of the other features.\n",
    "\n",
    "An iterative imputation is built thus the maximum number of iteration must be specified.\n",
    "\n",
    "We can use IterativeImputer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "preprocessor = IterativeImputer(max_iter=10, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.array(df['Cupcake']).reshape(-1,1)\n",
    "X2 = np.array(df.index).reshape(-1,1)\n",
    "X = np.hstack((X1, X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterativeImputer(random_state=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prep = preprocessor.transform(X)\n",
    "df['Cupcake_multicariate'] = np.hsplit(X_prep,2)[0].reshape(1,-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mese                      2006-03\n",
       "Cupcake                       NaN\n",
       "Cupcake_univariate      50.079208\n",
       "Cupcake_multicariate    28.631082\n",
       "Name: 26, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mese                      2004-02\n",
       "Cupcake                       NaN\n",
       "Cupcake_univariate      50.079208\n",
       "Cupcake_multicariate    21.610078\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nearest neighbors imputation\n",
    "\n",
    "- This method fills missing values using the k-nearest neigbors approach.\n",
    "- Each missing value is calculated using values from n_neigbors nearest neigbors that have a value.\n",
    "- We can use the KNNImputer class of the scikit_learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "preprocessor = KNNImputer(n_neighbors=5, weights=\"distance\")\n",
    "preprocessor.fit(X)\n",
    "X_prep = preprocessor.transform(X)\n",
    "df['Cupcake_knn'] = np.hsplit(X_prep, 2)[0].reshape(1,-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mese                      2006-03\n",
       "Cupcake                       NaN\n",
       "Cupcake_univariate      50.079208\n",
       "Cupcake_multicariate    28.631082\n",
       "Cupcake_knn                  10.7\n",
       "Name: 26, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mese                      2004-02\n",
       "Cupcake                       NaN\n",
       "Cupcake_univariate      50.079208\n",
       "Cupcake_multicariate    21.610078\n",
       "Cupcake_knn              4.918919\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the KNNImputer produces the nearest values to those in the original dataset (10 for position 26 and 5 for position 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is normalization\n",
    "\n",
    "Normalization is a process of scaling down data.\n",
    "\n",
    "Usually while normalizing we change the scale of the data to fall between 0 - 1.\n",
    "\n",
    "#### What is need for normalization\n",
    "\n",
    "It makes to understand the importance of each feature easily, when looking at the model weights.\n",
    "\n",
    "It also makes the process training less sentive to the scale of features.\n",
    "\n",
    "The process of making features more suitable for training by rescaling is called feature scaling.\n",
    "\n",
    "\n",
    "Normalization in python\n",
    "\n",
    "1. normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49 48 60 15 28 99 98  4 17 92 62 67 47 72 87]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "arr = np.random.randint(100, size=(15))\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.19874903 0.19469293 0.24336616 0.06084154 0.11357087 0.40155416\n",
      "  0.39749806 0.01622441 0.06895374 0.37316144 0.25147836 0.27175888\n",
      "  0.19063682 0.29203939 0.35288093]]\n"
     ]
    }
   ],
   "source": [
    "arr_norm = preprocessing.normalize([arr])\n",
    "print(arr_norm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. normalize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance_data = pd.read_csv('../Datasets/insurance2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02439715 0.02953017 0.02885684 ... 0.03222348 0.02256081 0.02542026]]\n"
     ]
    }
   ],
   "source": [
    "bmi = np.array(insurance_data['bmi'])\n",
    "normalized_bmi = preprocessing.normalize([bmi])\n",
    "print(normalized_bmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_data = preprocessing.normalize(insurance_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data\n",
      "    age  sex     bmi  children  smoker  region      charges  insuranceclaim\n",
      "0   19    0  27.900         0       1       3  16884.92400               1\n",
      "1   18    1  33.770         1       0       2   1725.55230               1\n",
      "2   28    1  33.000         3       0       2   4449.46200               0\n",
      "3   33    1  22.705         0       0       1  21984.47061               0\n",
      "4   32    1  28.880         0       0       1   3866.85520               1\n",
      "5   31    0  25.740         0       0       2   3756.62160               0\n",
      "6   46    0  33.440         1       0       2   8240.58960               1\n",
      "7   37    0  27.740         3       0       1   7281.50560               0\n",
      "8   37    1  29.830         2       0       0   6406.41070               0\n",
      "9   60    0  25.840         0       0       1  28923.13692               0\n",
      "Normalized Data\n",
      "         age       sex       bmi  children    smoker    region   charges  \\\n",
      "0  0.012472  0.000000  0.024397  0.000000  0.060412  0.043732  0.025698   \n",
      "1  0.011816  0.038462  0.029530  0.016791  0.000000  0.029154  0.002626   \n",
      "2  0.018380  0.038462  0.028857  0.050372  0.000000  0.029154  0.006772   \n",
      "3  0.021662  0.038462  0.019854  0.000000  0.000000  0.014577  0.033460   \n",
      "4  0.021006  0.038462  0.025254  0.000000  0.000000  0.014577  0.005885   \n",
      "5  0.020349  0.000000  0.022508  0.000000  0.000000  0.029154  0.005717   \n",
      "6  0.030196  0.000000  0.029242  0.016791  0.000000  0.029154  0.012542   \n",
      "7  0.024288  0.000000  0.024257  0.050372  0.000000  0.014577  0.011082   \n",
      "8  0.024288  0.038462  0.026085  0.033581  0.000000  0.000000  0.009750   \n",
      "9  0.039386  0.000000  0.022596  0.000000  0.000000  0.014577  0.044020   \n",
      "\n",
      "  insuranceclaim  \n",
      "0       0.035737  \n",
      "1       0.035737  \n",
      "2       0.000000  \n",
      "3       0.000000  \n",
      "4       0.035737  \n",
      "5       0.000000  \n",
      "6       0.035737  \n",
      "7       0.000000  \n",
      "8       0.000000  \n",
      "9       0.000000  \n"
     ]
    }
   ],
   "source": [
    "insurance_data = pd.read_csv('../Datasets/insurance2.csv')\n",
    "norm_data = preprocessing.normalize(insurance_data, axis=0)\n",
    "norm_df = pd.DataFrame(norm_data, columns=[insurance_data.columns])\n",
    "print('Original Data\\n', insurance_data.head(10))\n",
    "print('Normalized Data\\n', norm_df.head(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data\n",
      "    age  sex     bmi  children  smoker  region      charges  insuranceclaim\n",
      "0   19    0  27.900         0       1       3  16884.92400               1\n",
      "1   18    1  33.770         1       0       2   1725.55230               1\n",
      "2   28    1  33.000         3       0       2   4449.46200               0\n",
      "3   33    1  22.705         0       0       1  21984.47061               0\n",
      "4   32    1  28.880         0       0       1   3866.85520               1\n",
      "5   31    0  25.740         0       0       2   3756.62160               0\n",
      "6   46    0  33.440         1       0       2   8240.58960               1\n",
      "7   37    0  27.740         3       0       1   7281.50560               0\n",
      "8   37    1  29.830         2       0       0   6406.41070               0\n",
      "9   60    0  25.840         0       0       1  28923.13692               0\n",
      "MinMaxScaler Data\n",
      "         age  sex       bmi children smoker    region   charges insuranceclaim\n",
      "0  0.043478  0.0  0.642454      0.0    2.0  2.000000  0.503222            2.0\n",
      "1  0.000000  2.0  0.958300      0.4    0.0  1.333333  0.019272            2.0\n",
      "2  0.434783  2.0  0.916868      1.2    0.0  1.333333  0.106230            0.0\n",
      "3  0.652174  2.0  0.362927      0.0    0.0  0.666667  0.666020            0.0\n",
      "4  0.608696  2.0  0.695184      0.0    0.0  0.666667  0.087631            2.0\n",
      "5  0.565217  0.0  0.526231      0.0    0.0  1.333333  0.084112            0.0\n",
      "6  1.217391  0.0  0.940543      0.4    0.0  1.333333  0.227259            2.0\n",
      "7  0.826087  0.0  0.633844      1.2    0.0  0.666667  0.196641            0.0\n",
      "8  0.826087  2.0  0.746301      0.8    0.0  0.000000  0.168704            0.0\n",
      "9  1.826087  0.0  0.531612      0.0    0.0  0.666667  0.887531            0.0\n"
     ]
    }
   ],
   "source": [
    "insurance_data = pd.read_csv('../Datasets/insurance2.csv')\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(0, 2))\n",
    "norm = scaler.fit_transform(insurance_data)\n",
    "norm_df = pd.DataFrame(norm, columns=[insurance_data.columns])\n",
    "print('Original Data\\n', insurance_data.head(10))\n",
    "print('MinMaxScaler Data\\n', norm_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
